## Reinforcement Learning
### Asynchronous Methods for Deep Reinforcement Learning [[]]()
  - Volodymyr Mnih et al.
### Massively Parallel Methods for Deep Reinforcement Learning [[]]()
  - Arun Nair et al.
### PGQ: Combining policy gradient and Q-learning [[]]()
  - Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, Volodymyr Mnih
### Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic [[review ICLR 2017]]()
  - Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, Sergey Levine
### Action-Conditional Video Prediction using Deep Networks in Atari Games [[NIPS 2015]](https://arxiv.org/pdf/1507.08750v2.pdf)
  - Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, Satinder Singh
  
### Mastering the game of Go with deep neural networks and tree search [[Nature 2016]] (http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf)
  - David Silver et al.
  - Use RL to play Go
  - Core algorithm is monte carlo tree search using a trained policy network to get action probabilities
  - Leaf nodes are evaluated using both a fast rollout policy network and a trained value network
  
### Dueling Network Architectures for Deep Reinforcement Learning [[ICML 2016]](https://arxiv.org/pdf/1511.06581.pdf)
  - Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas
  - Two-stream DQN, each representing V(value function) and A(advantage function)
  - Elinimates the instability of adding two numbers of different scale(V is usually much larger than A)
  - Updates actions more frequently than a single-stream DQN, where only a single Q value is updated for each observation
  - Implicitly splits the credit assignment problem into a recursive binary problem of "now or later"
  - Arguably the first major breakthrough in network architectures specifically for deep RL
  - Only works for finite action spaces
  
### Deep Exploration via Bootstrapped DQN [[NIPS 2016]](https://arxiv.org/pdf/1602.04621v3.pdf)
  - Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy

### Continuous Control with Deep Reinforcement Learning [[ICLR 2016]](https://arxiv.org/pdf/1509.02971)
  - Timothy P. Lillicrap et al.
  - Videos available [here](https://goo.gl/J4PIAz)
  - Suggests DDPG, which improves the actor-critic algorithm in [Deterministic Policy Gradient Algorithms](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/policy_gradient.md#deterministic-policy-gradient-algorithms-icml-2014) by using a DQN as the critic
  - Empirically shown to be more efficient than DQN
  
### High-dimensional Continuous Control Using Generalized Advantage Functions [[ICLR 2016]](https://arxiv.org/pdf/1506.02438)
  - John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, Pieter Abbeel
  - Derives a class of estimators(GAE) of the advantage function, parameterized by two real numbers in [0,1]
  - Empirical performance of TRPO+GAE is better than TRPO in some tasks
  
### Connecting Generative Adversarial Networks and Actor-Critic Methods [[arxiv 2016]] (https://arxiv.org/pdf/1610.01945v2.pdf)
  - David Pfau, Oriol Vinyals
  - Constructs an Actor-Critic architecture that is equivalent to GAN: state is randomly chosen between a real image and an image generated by the actor, action is setting every pixel in an image, reward if 1 if real image and 0 if synthetic image
  - Cross-examines the approaches used to stabilize GANs and AC architectures
  
### Prioritized Experience Replay [[ICLR 2016]](https://arxiv.org/pdf/1511.05952.pdf)
  - Tom Schaul, John Quan, Ioannis Antonoglou, David Silver
  - Samples (s,a,r,s') tuples with probability proportional to their TD error
  - Uses a 'sum-tree' data structure to perform this quickly, where the value of a parent node is the sum of its children
  - Uses importance sampling weights to counteract the change in state distribution
  - Inspired by prioritized sweeping
  
### Deep Reinforcement Learning with Double Q-Learning [[AAAI 2016]](https://arxiv.org/pdf/1509.06461.pdf)
  - Hado van Hasselt, Arthur Guez, David Silver
  - Points out that once [DQN](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/q_learning.md#playing-atari-with-deep-reinforcement-learning-nips-2014-deep-learning-workshop) overestimates a Q value, the overestimation 'spills over' to states that precede it
  - Uses different Q networks for action selection and evaluation
  - Empirically shows that Double DQN outperforms DQN
  
### Human-level Control Through Deep Reinforcement Learning [[Nature 2015]](http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf)
  - Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg & Demis Hassabis
  - Appeared earlier in [NIPS 2013 Deep Learning Workshop](https://arxiv.org/pdf/1312.5602.pdf)
  - Proposes using a deep convolutional network for Q-learning
  - Uses experience replay(similar to  [Neural Fitted Q Iteration](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/q_learning.md#neural-fitted-q-iteration---first-experiences-with-a-data-efficient-neural-reinforcement-learning-method-ecml-2005)), storing only the last N experience tuples while using an epsilon greedy policy
  - Points out that a scheme similar to experience replay happens in the hippocampus of the mammalian brain
  - Difference with [Neural Fitted Q Iteration](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/q_learning.md#neural-fitted-q-iteration---first-experiences-with-a-data-efficient-neural-reinforcement-learning-method-ecml-2005)) is that this is an off-policy learning algorithm where the network determines the policy
  - Periodically fixes network parameters for stability
  - Mentions multiple times that a scheme similar to prioritized sweeping would speed up convergence, which is exactly [Prioritized Experience Replay](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/q_learning.md#prioritized-experience-replay-iclr-2016)
  
### Trust Region Policy Optimization [[ICML 2015]](https://arxiv.org/pdf/1502.05477)
  - John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, Pieter Abbeel
  - Builds on [Approximately Optimal Approximate Reinforcement Learning](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/policy_gradient.md#approximately-optimal-approximate-reinforcement-learning-icml-2002)
  - Instead of using a linear mixture, TRPO uses average KL divergence to ensure that the next policy is sufficiently close to current policy(in practice, approximate L linearly and KL divergence quadratically)
  - The natural policy gradient has the same direction as TRPO; the difference is that TRPO chooses a step size based on the trust region defined by KL divergence
  - Empirical performance is comparable to state of the art in both robotic locomotion and atari games
  
### Deep Recurrent Q-Learning for Partially Observable MDPs [[arxiv 2015]](https://arxiv.org/pdf/1507.06527v3.pdf)
  - Matthew Hausknecht, Peter Stone
  - Attaches an LSTM to a standard DQN, so that it can learn even with only timestep as input
  - Experiments show that this is superior to DQN in an env where at each timestep, the state is obscured with probability 1/2
  
### Deterministic Policy Gradient Algorithms [[ICML 2014]](http://jmlr.org/proceedings/papers/v32/silver14.pdf)
  - David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller
  - Derives a gradient for deterministic policies(assuming a continuous action space)
  - Proves(under mild regularity conditions) that all previously developed machinery for stochastic policy gradients(i.e. compatible function approximation, actor-critic, natural gradients, and episodic/batch methods) are applicable to determinisic policy gradients.
  - All proofs are in [a seperate document](http://jmlr.org/proceedings/papers/v32/silver14-supp.pdf)

### Reinforcement learning of motor skills with policy gradients [[Neural Networks 2008]](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/Neural-Netw-2008-21-682_4867[0].pdf)
  - Jan Peters, Stefan Schaal
  - An extensive survey of policy gradient methods
  - Covers naive finite-difference methods, REINFORCE, NAC(natural actor-critic)
  - NAC is shown to be the state of the art
  
### Natural Actor-Critic [[Neurocomputing 2008]] (http://ac.els-cdn.com/S0925231208000532/1-s2.0-S0925231208000532-main.pdf?_tid=5001927e-69ce-11e6-87f9-00000aacb35f&acdnat=1472024730_f49e79f185266d4824826941cec13967)
  - Jan Peters, Stefan Schaal
  - Proves that the weight vector discussed in [A Natural Policy Gradient](https://github.com/yoonholee/Reinforcement-Learning-Survey/blob/master/policy_gradient.md#a-natural-policy-gradient-nips-2002) is in fact the natural gradient, rather than just a gradient defined by an average of point fisher information matrices
  - Suggests a actor-critic style algorithm using the natural gradient
  
### A Natural Policy Gradient [[NIPS 2002]](http://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf)
  - Sham Kakade
  - Parameterizes Q(s,a) as a weighted sum of log p(a|s)
  - Proves that the weight vector(above) is the direction of steepest descent with respect to the expectation of the fisher information matrix(natural policy gradient)
  - Suggests a REINFORCE-style algorithm using the natural gradient
  
### Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method [[ECML 2005]](http://ml.informatik.uni-freiburg.de/_media/publications/rieecml05.pdf)
  - Martin Riedmiller
  - Proposes using a neural network for Q-learning
  - Input is a set of (s,a,s') and the neural network does RProp until convergence
  - Closer to supervised learning than reinforcement learning, since the algorithm never acts
 
### Approximately Optimal Approximate Reinforcement Learning [[ICML 2002]](https://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf)
  - Sham Kakade, John Langford
  - Points out the inefficiency of policy gradients using two example MDPs(section 3.2)
  - Derives a conservative policy iteration scheme that finds a policy that is almost optimal(within epsilon) in polynomial(w.r.t. epsilon) time
  - The key idea is that we can get a provably improved policy by using a linear mixture between the current policy and the greedily improved policy

### Convergence of Stochastic Iterative Dynamic Programming Algorithms [[NIPS 1994]](http://papers.nips.cc/paper/764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.pdf)
  - Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh
  - Proves the convergence of Q-Learning to optimal Q values given some mild regulatory conditions
  - Gives a similar proof for TD(lambda)
